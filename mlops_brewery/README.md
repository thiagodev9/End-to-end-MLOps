# ğŸº MLOps Brewery Project

## ğŸ‡§ğŸ‡· DescriÃ§Ã£o do Projeto

Este Ã© um projeto de MLOps criado para fins de portfÃ³lio profissional. O objetivo Ã© demonstrar, de forma prÃ¡tica e simples, como construir um pipeline de dados e MLOps usando PySpark, Docker, MLflow, CI/CD e Infrastructure as Code (Terraform).

O projeto realiza a ingestÃ£o paginada de dados da API pÃºblica Open Brewery DB, processa os dados com PySpark, salva os resultados em formato Parquet e registra experimentos no MLflow. Toda a aplicaÃ§Ã£o roda em containers Docker e possui automaÃ§Ãµes de CI/CD.

## ğŸ‡ºğŸ‡¸ Project Description

This is an  MLOps project created for a professional portfolio. The goal is to show how to build a simple data and MLOps pipeline using PySpark, Docker, MLflow, CI/CD, and Terraform.

The project ingests paginated data from the public Open Brewery DB API, processes the data with PySpark, saves it in Parquet format, and tracks experiments using MLflow. Everything runs inside Docker containers.

## ğŸ§± Arquitetura do Projeto

### ğŸ‡§ğŸ‡· VisÃ£o Geral

- API pÃºblica (Open Brewery DB)
- IngestÃ£o paginada com PySpark
- Armazenamento em Parquet (Data Lake)
- MLflow para rastreamento de experimentos
- Docker para containerizaÃ§Ã£o
- GitHub Actions para CI
- Terraform para infraestrutura na AWS

### ğŸ‡ºğŸ‡¸ Overview

- Public API (Open Brewery DB)
- Paginated ingestion with PySpark
- Parquet storage (Data Lake)
- MLflow for experiment tracking
- Docker containers
- GitHub Actions for CI
- Terraform for AWS infrastructure

## ğŸ“‚ Estrutura do Projeto

```
mlops_brewery/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingest.py
â”‚   â”œâ”€â”€ preprocess.py
â”‚   â”œâ”€â”€ train.py
â”‚   â””â”€â”€ run_pipeline.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ breweries_processed.csv
â”‚   â””â”€â”€ breweries_raw_parquet/
â”œâ”€â”€ infra/
â”‚   â”œâ”€â”€ provider.tf
â”‚   â”œâ”€â”€ main.tf
â”‚   â”œâ”€â”€ variables.tf
â”‚   â””â”€â”€ outputs.tf
â””â”€â”€ databricks/
    â”œâ”€â”€ jobs/
    â”‚   â””â”€â”€ brewery_ingestion_job.json
    â””â”€â”€ workflows/
        â””â”€â”€ brewery_workflow.md
```
## ğŸ”„ Pipeline de Dados

### ğŸ‡§ğŸ‡· Etapas

1. Consumo da API Open Brewery DB
2. PaginaÃ§Ã£o dos dados (200 registros por pÃ¡gina)
3. ConversÃ£o para DataFrame PySpark
4. Tratamento de tipos de dados
5. Escrita em formato Parquet

### ğŸ‡ºğŸ‡¸ Steps

1. Consume Open Brewery DB API
2. Paginated data ingestion
3. Convert to PySpark DataFrame
4. Data type handling
5. Save data as Parquet

## ğŸ³ Docker

### ğŸ‡§ğŸ‡· Uso

O Docker garante que o projeto rode da mesma forma em qualquer ambiente.

```bash
docker compose up --build
```

### ğŸ‡ºğŸ‡¸ Usage

Docker makes the project run the same way in any environment.

## ğŸ“Š MLflow

### ğŸ‡§ğŸ‡· FunÃ§Ã£o

- Registrar execuÃ§Ãµes
- Monitorar mÃ©tricas
- Versionar experimentos

MLflow roda em: http://localhost:5000

### ğŸ‡ºğŸ‡¸ Purpose

- Track runs
- Monitor metrics
- Version experiments

## ğŸ” CI/CD (GitHub Actions)

### ğŸ‡§ğŸ‡· O pipeline faz

- Checkout do cÃ³digo
- Instala dependÃªncias
- Lint bÃ¡sico
- Build da imagem Docker
- ExecuÃ§Ã£o de teste do pipeline

### ğŸ‡ºğŸ‡¸ CI/CD Does

- Code checkout
- Install dependencies
- Basic lint
- Build Docker image
- Test pipeline execution

## â˜ï¸ Terraform (Infrastructure as Code)

### ğŸ‡§ğŸ‡· Recursos criados

- S3 (Data Lake)
- EC2 (Docker / Spark / MLflow)
- Security Group (Firewall)

### ğŸ‡ºğŸ‡¸ Resources

- S3 bucket (Data Lake)
- EC2 instance
- Security Group (Firewall)

## ğŸ” SeguranÃ§a

### ğŸ‡§ğŸ‡· Security Group

- Porta 22 (SSH)
- Porta 5000 (MLflow)

### ğŸ‡ºğŸ‡¸ Security

- Port 22 for SSH
- Port 5000 for MLflow

## ğŸ§  Aprendizados

### ğŸ‡§ğŸ‡·

- PySpark na prÃ¡tica
- Docker para pipelines de dados
- Fundamentos de MLOps
- CI/CD com GitHub Actions
- Infraestrutura com Terraform

### ğŸ‡ºğŸ‡¸

- PySpark basics
- Docker for data pipelines
- MLOps fundamentals
- CI/CD with GitHub Actions
- Infrastructure with Terraform

## ğŸ¯ Objetivo Profissional

### ğŸ‡§ğŸ‡·

Este projeto foi criado para demonstrar competÃªncias exigidas em vagas de MLOps Engineer / Data Engineer, com foco em boas prÃ¡ticas, automaÃ§Ã£o e cloud.

### ğŸ‡ºğŸ‡¸

This project shows skills required for MLOps Engineer / Data Engineer roles.

---

## ğŸ‘¤ Autor

**Thiago Camargo**

Data Engineer | MLOps | Python | Cloud

## âš ï¸ ObservaÃ§Ã£o

Este projeto Ã© educacional e utiliza dados pÃºblicos apenas para fins de estudo.